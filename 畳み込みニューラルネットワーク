
https://ja.wikipedia.org/wiki/%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF
画像処理など

1．神経細胞
入力、Weight、活性化関数、出力

a[2] = f(a[1] = W*a[0] + b)

2．活性化関数
sigmoid、tanh、ReLUはよく使われています。

■sigmoid
g(z) = 1 / ( 1 + e^(-z) )
この関数は任意の実数を0～1に圧縮することです。大きいければ大きいほど、1になり、逆に0になります。
二項分類によく使われている関数となります。例えば、0.9は90%の概率の意味合いです。

pythonでやってみると、
import numpy as np
y = 1.0 / (1.0 + np.exp(-x))

しかし、精度の問題もあるそうですが、lr1モジュールのsigmoid関数を使った方がいいです。
参照リンク：http://www.kamishima.net/mlmpyja/lr/sigmoid.html

■ReLU

3．ニューラルネットワーク
個々神経細胞を組み合わせると、ニューラルネットワークを建てます。

入力層（Input layer） 出力層（Output layer）隠れ層（Hidden layer）

各層は単一または複数の神経細胞が含まれ、層の出力は次の層の入力データとして使われます。
隠れ層も複数のある可能ですが、異なる層にはつながりますが、同一層にはリンクはありません。
